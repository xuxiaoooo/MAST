## Attention-based Multiple Time Series fusion model (MAST), designed for EEG

![Image text](https://github.com/xuxiaoooo/MAST/blob/main/draw/MAST.jpg)

Inspired by:
```
@article{wan2023eegformer,
  title={EEGformer: A transformer--based brain activity classification method using EEG signal},
  author={Wan, Zhijiang and Li, Manyu and Liu, Shichang and Huang, Jiajin and Tan, Hai and Duan, Wenfeng},
  journal={Frontiers in Neuroscience},
  volume={17},
  pages={1148855},
  year={2023},
  publisher={Frontiers}
}
```
```
@article{zhang2023sageformer,
  title={SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting},
  author={Zhang, Zhenwei and Wang, Xin and Gu, Yuantao},
  journal={arXiv preprint arXiv:2307.01616},
  year={2023}
}
```
```
@article{dong2023mesh,
  title={Mesh-MLP: An all-MLP Architecture for Mesh Classification and Semantic Segmentation},
  author={Dong, Qiujie and Xu, Rui and Gong, Xiaoran and Wang, Zixiong and Chen, Shuangmin and Xin, Shiqing and Tu, Changhe},
  journal={arXiv preprint arXiv:2306.05246},
  year={2023}
}
```
```
@inproceedings{jiang2023elastic,
  title={Elastic Graph Transformer Networks for EEG-Based Emotion Recognition},
  author={Jiang, Wei-Bang and Yan, Xu and Zheng, Wei-Long and Lu, Bao-Liang},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}
```
